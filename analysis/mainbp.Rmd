---
title: "mainBP"
output: html_document
---

AT SOME POINT, EXPLAIN THAT AN INTEGRAL ON PROBABILITIES IS AN AVERAGE, OR THAT A SUM OVER PROBABILITIES IS AN AVERAGE

There will be a few boxes with a further explanation of some topics, but skip them if you prefer.

| BOX ABOUT A GIVEN TOPIC |
|-------------------------|
| END OF BOX              |
|                         |

haven't read on last pass of info_theory.Rmd:

-   anything outside of ben lambert or gelmans stuff

-   really anything from kl divergence outside the general idea that ben lambert gives or the part that gelman names it briefly.

index:

-   why are we interested in prediction accuracy? (good paragraph from gelman).

-   3 or 2. how do we measure predictive accuracy (general probability scoring rules, then logscores). How we measure predictive accuracy of our bayesian model taking into account the two sources of uncertainty.

+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Kullback-Liebler Divergence                                                                                                                                                                                                                         |
+=====================================================================================================================================================================================================================================================+
| A few properties of the KL-Divergence:                                                                                                                                                                                                              |
|                                                                                                                                                                                                                                                     |
| -   It is always non-negative $D_{KL}(P || Q) \geq 0$, a result known as "Gibbs inequality", with $D_{KL}(P || Q)$ equal to zero iff $P = Q$ almost everywhere.                                                                                     |
|                                                                                                                                                                                                                                                     |
| -   No upper-bound exists for the general case. However, it is shown that if $P$ and $Q$ are two discrete probability distributions built by distributing the same discrete quantity, then the maximum value of $D_{KL}(P || Q)$ can be calculated. |
|                                                                                                                                                                                                                                                     |
| -   KL divergence remains well-defined for continuous distributions, and furthermore is invariant under parameter transformations.                                                                                                                  |
|                                                                                                                                                                                                                                                     |
| -   KL divergence is additive for independent distributions, in much the same way as Shannon entropy.                                                                                                                                               |
|                                                                                                                                                                                                                                                     |
| Not understood deeply enough yet to give a thorough explanation on this topic, more on this later.                                                                                                                                                  |
|                                                                                                                                                                                                                                                     |
| EXPLAIN KL DIVERGENCE RELATION TO MUTUAL INFORMATION.                                                                                                                                                                                               |
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                                                                                                                                                                                                                                                     |
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Prior vs Posterior Predictive distribution                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
+============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================+
| It's important to note that we have both a **prior** and a **posterior** predictive distribution.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| The **prior** predictive distribution is just the distribution of data which we think we are going to obtain before we actually see the data $y$, thus the $prior\ predictive\ distribution = p(y)$. This is based on our prior sort of knowledge about the situation. How can we calculate it? the idea is that we're trying to obtain is the probability of our data $y$. This is a marginal probability and we know that we can get to a marginal probability by integrating out all dependences, in this case $\theta$, of our joint probability $p(y, \theta)$ across all the range of values theta can take ($\theta \in \Theta$) . So by integrating this out we're removing all this theta dependence and we're just left with the marginal probability $p(y)$. So |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| $$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| p(y) = \int_{\theta \in \Theta} p(y,\theta)d\theta                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| $$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| and through the product rule of probability, which states that $p(A, B) = p(A|B)p(B)$, we get:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| $$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| p(y) = \int_{\theta \in \Theta} p(y,\theta)d\theta = \int_{\theta \in \Theta} p(y|\theta)p(\theta)d\theta.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| $$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| where $p(y|\theta)$ is the likelihood and $p(\theta)$ the prior.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| And with the posterior predictive distribution we essentially mean what value of data we would expect to obtain (that is, what our model would suggest after using the first observed data for fitting said model) if were to repeat the observation. We will call this new observation of data, $y'$. So how do we calculate this posterior predictive distribution? The idea is that we are trying to calculate the probability of certain value of the new data $y'$ given that we have observed the current data $y$ (the one used to fit the model), that is, $p(y'|y)$. We can get this conditional probability by integrating out the joint probability of $y'$ and $\theta$, acrross al range of theta ($\Theta$). Thus,                                           |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| $$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| p(y'|y) = \int_{\theta \in \Theta} p(y',\theta |y)d\theta                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| $$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| and through the chainrule of probability?, which states that $p(A, B |C) = p(A|B, C) p(B|C)$, we get:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| $$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| p(y'|y) = \int_{\theta \in \Theta} p(y',\theta |y)d\theta = \int_{\theta \in \Theta} p(y'|\theta, y) p(\theta|y)d\theta                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| $$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| and normally when you condition on $\theta$, our new observation $y'$ is independent of the current observation $y$ (that is, once you know $\theta$, $y$ doesn't provide any extra information about $y'$ ) and thus we can eliminate the dependency of $y'$ on $y$, so we get:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| $$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| p(y'|y) = \int_{\theta \in \Theta} p(y'|\theta) p(\theta|y)d\theta                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| $$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| where $p(y'|\theta)$ is the likelihood and $p(\theta|y)$ is the posterior distribution.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

Given a posterior distribution $p(\theta | \pmb{x})$, we can produce a posterior predictive distribution, which for a single new data point equals $p(x^{new} | \pmb{x})$ (m' more detail on this on here NOTE), and this just tells us what is our sort of best prediction or distribution which ecompasses what we think about predictions for the next data point given that we have observed a data vector $\pmb{x}$. This distribution $p(x^{new} | \pmb{x})$ is an approximation to some unkown true data generating process which, we can call $f(x^{new})$. Ideally, if we knew what the distribution $f(x^{new})$ was, we could compare it with our posterior predictive distribution $p(x^{new} | \pmb{x})$. The best way we could compare this two distributions (if we knew both of them), is with what is called the Kullback-Leibler divergence ($D_{KL}$, also known as "relative entropy"), which is just a general measure of the difference between two distributions.

-   2 or 3. idea of posterior predictive distribution vs true but unknown distribution. KL divergence, general idea, some properties (more things in a box). Maybe in another box the note that we have both the prior and a posterior predictive distribution.

+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Information Criteria (AIC, DIC, WAIC).                                                                                                                                                      |
+=============================================================================================================================================================================================+
| general thing of what they aim to do (measure predictive accuracy) using what (data used to fit the model) and how they do it (some sort of measure of the log likelihood minus a penalty). |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                                                                                                                                                                                             |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

what is IC doing?

-   In the case of a hierarchical model, What are we trying to predict? in group or another group? differences in the posterior predictive distribution and why we are not bothered by the non-uniqueness of the posterior predictive distribution.

-   what should we do if we are doing LOO-CV. (for illustration and to stand the difference with LFO-CV).

+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| JAGS output and the logdensity functions                                                                                                                                                                                                                                                                                                        |
+=================================================================================================================================================================================================================================================================================================================================================+
| Straight from JAGS user manual 2017 ([\<https://sourceforge.net/projects/mcmc-jags/files/Manuals/4.x/\>](https://sourceforge.net/projects/mcmc-jags/files/Manuals/4.x/){.uri}).                                                                                                                                                                 |
|                                                                                                                                                                                                                                                                                                                                                 |
| START OF EXCERPT                                                                                                                                                                                                                                                                                                                                |
|                                                                                                                                                                                                                                                                                                                                                 |
| 6.3. Functions associated with distribution                                                                                                                                                                                                                                                                                                     |
|                                                                                                                                                                                                                                                                                                                                                 |
| All distribution sin JAGS have a log density function associated withi them. For example, if variable $x$ has a normal distribution with mean $mu$ and precision $tau$:                                                                                                                                                                         |
|                                                                                                                                                                                                                                                                                                                                                 |
| $$                                                                                                                                                                                                                                                                                                                                              |
| x ~ \sim dnorm(mu, tau)                                                                                                                                                                                                                                                                                                                         |
| $$                                                                                                                                                                                                                                                                                                                                              |
|                                                                                                                                                                                                                                                                                                                                                 |
| then the log density of $x$ is given by                                                                                                                                                                                                                                                                                                         |
|                                                                                                                                                                                                                                                                                                                                                 |
| $$                                                                                                                                                                                                                                                                                                                                              |
| ldx \leftarrow logdensity.norm(x, mu, tau)                                                                                                                                                                                                                                                                                                      |
| $$                                                                                                                                                                                                                                                                                                                                              |
|                                                                                                                                                                                                                                                                                                                                                 |
| In general, if "dfoo" is the name of a distribution then the associated log density function is "logdensity.foo". The first argument of the log-density function is the sample value at which the log-density is evaluated and the remaining arguments are the parameters of the distribution <!--# posterior parameters i'd like to think -->. |
|                                                                                                                                                                                                                                                                                                                                                 |
| FINISH OF EXCERPT                                                                                                                                                                                                                                                                                                                               |
|                                                                                                                                                                                                                                                                                                                                                 |
| Maybe say that u r not entirely sure if this is giving u what u need (the "posterior" log likelihood or posterior predictive density), and that u still need to check JAGS codebase to make sure because it's not clearly explained in the user manual.                                                                                         |
|                                                                                                                                                                                                                                                                                                                                                 |
| search for the first time "density" comes up in the manual                                                                                                                                                                                                                                                                                      |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                                                                                                                                                                                                                                                                                                                                                 |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

-   also here state what do we need for computing loocv with the loo package and also check the loo package for how to use with different prediction objectives and thus predictive distributions. (a matrix with log likelihoods?; state the problem of jags documentation that it says it can give you the loglikelihood (which is what the predictive distribution is sometimes called) but it's not specified if its giving you the prior or posterior distribution. to solve this, see if the chapter where that definition is gives you a hint, or can you run the model without data to see if the log likelihood appears? or maybe use different data but same priors and see if there's a difference between the matrix jags retrieves, maybe add a random value to each data point and check if the logposterior is the same, but the problem here is that there's always some sampling variability, or is the set.seed what completely avoids this? maybe run the model many times with the same seed, save the log likelihoods and then compare them with that package to compare datasets.).

-   what should we do if we are doing LFO-CV.

+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| log-sum-exp trick                                                                                                                                                                                                                           |
+=============================================================================================================================================================================================================================================+
| [**from info theory book**]{.underline}:                                                                                                                                                                                                    |
|                                                                                                                                                                                                                                             |
| logs turn multiplication into sums. m' and thats much stable due to? numerical over and underflow?                                                                                                                                          |
|                                                                                                                                                                                                                                             |
| ------------------------------------------------------------------------                                                                                                                                                                    |
|                                                                                                                                                                                                                                             |
| [**from mathmatical monk on logsumexp**]{.underline} [\<https://www.youtube.com/watch?v=-RVM21Voo7Q&t=2s\>](https://www.youtube.com/watch?v=-RVM21Voo7Q&t=2s){.uri}                                                                         |
|                                                                                                                                                                                                                                             |
| underflow: when you are trying to represent a number on a computer and the number is to small to represent. When you get a super small numer, it automatically sets to 0 in some programming languages <!--# this is what happens in R -->. |
|                                                                                                                                                                                                                                             |
| **underflow can be fixed with the log-sum-exp trick**.                                                                                                                                                                                      |
|                                                                                                                                                                                                                                             |
| underflow tends to happen when you multiply many probabilities together or when you many data and compute the probability of a given event.                                                                                                 |
|                                                                                                                                                                                                                                             |
| When you want to represent a very small number you can use logs.                                                                                                                                                                            |
|                                                                                                                                                                                                                                             |
| If u have Z = P(A) \* P(B) \* P(C), then                                                                                                                                                                                                    |
|                                                                                                                                                                                                                                             |
| Z = exp(logP(A) + logP(B) + logP(C)), but this way Z will still be very small, so we need log(Z), then                                                                                                                                      |
|                                                                                                                                                                                                                                             |
| log(Z) = log (exp(logP(A) + logP(B) + logP(C)))                                                                                                                                                                                             |
|                                                                                                                                                                                                                                             |
| If we have a sum, such as $\sum_{i=1}^N P(A|Bi) * P(Bi) = Z$, then                                                                                                                                                                          |
|                                                                                                                                                                                                                                             |
| Z = sum( exp( logP(A\|Bi) + logP(Bi) ) ), and                                                                                                                                                                                               |
|                                                                                                                                                                                                                                             |
| log(Z) = log( sum( exp( logP(A\|Bi) + logP(Bi) ) ) )                                                                                                                                                                                        |
|                                                                                                                                                                                                                                             |
| but theres a problem because if for example P(Bi) is small, then logP(Bi) is going to be a negative but big in module number, and taking the exp of that will end up underflowing to 0. So a fix to that is...                              |
|                                                                                                                                                                                                                                             |
| TO BE CONTINUED (LEFT AT MIN 6:40)                                                                                                                                                                                                          |
|                                                                                                                                                                                                                                             |
| ------------------------------------------------------------------------                                                                                                                                                                    |
|                                                                                                                                                                                                                                             |
| [**from luis serrano's vid**]{.underline}: [\<https://www.youtube.com/watch?v=9r7FIXEAGvs\>](https://www.youtube.com/watch?v=9r7FIXEAGvs){.uri}                                                                                             |
|                                                                                                                                                                                                                                             |
| products are small and volatile, sums are good                                                                                                                                                                                              |
|                                                                                                                                                                                                                                             |
| **log is a function that turns products into sums due to the following identity**: $log(a*b) = log(a) + log(b)$.                                                                                                                            |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                                                                                                                                                                                                                                             |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
