<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Untitled</title>

<script src="site_libs/header-attrs-2.11/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<link rel="icon" href="https://github.com/workflowr/workflowr-assets/raw/master/img/reproducible.png">
<!-- Add a small amount of space between sections. -->
<style type="text/css">
div.section {
  padding-top: 12px;
}
</style>



<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">JAGS_LFO</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="license.html">License</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/franfram/JAGS_LFO">
    <span class="fa fa-github"></span>
     
    Source code
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Untitled</h1>

</div>


<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-report" data-toggle="collapse" data-target="#workflowr-report">
<span class="glyphicon glyphicon-list" aria-hidden="true"></span> workflowr <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span>
</button>
</p>
<div id="workflowr-report" class="collapse">
<ul class="nav nav-tabs">
<li class="active">
<a data-toggle="tab" href="#summary">Summary</a>
</li>
<li>
<a data-toggle="tab" href="#checks"> Checks <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span> </a>
</li>
<li>
<a data-toggle="tab" href="#versions">Past versions</a>
</li>
</ul>
<div class="tab-content">
<div id="summary" class="tab-pane fade in active">
<p>
<strong>Last updated:</strong> 2021-10-23
</p>
<p>
<strong>Checks:</strong> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> 1 <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span> 1
</p>
<p>
<strong>Knit directory:</strong> <code>JAGS_LFO/</code> <span class="glyphicon glyphicon-question-sign" aria-hidden="true" title="This is the local directory in which the code in this file was executed."> </span>
</p>
<p>
This reproducible <a href="http://rmarkdown.rstudio.com">R Markdown</a> analysis was created with <a
  href="https://github.com/jdblischak/workflowr">workflowr</a> (version 1.6.2). The <em>Checks</em> tab describes the reproducibility checks that were applied when the results were created. The <em>Past versions</em> tab lists the development history.
</p>
<hr>
</div>
<div id="checks" class="tab-pane fade">
<div id="workflowr-checks" class="panel-group">
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRMarkdownfilestronguncommittedchanges"> <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span> <strong>R Markdown file:</strong> uncommitted changes </a>
</p>
</div>
<div id="strongRMarkdownfilestronguncommittedchanges" class="panel-collapse collapse">
<div class="panel-body">
<p>The R Markdown file has unstaged changes. To know which version of the R Markdown file created these results, you’ll want to first commit it to the Git repo. If you’re still working on the analysis, you can ignore this warning. When you’re finished, you can run <code>wflow_publish</code> to commit the R Markdown file and build the HTML.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRepositoryversionstrongahrefhttpsgithubcomfranframJAGSLFOtreedbab5d599de6bbe2e8a43370a690018279cd557btargetblankdbab5d5a"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Repository version:</strong> <a href="https://github.com/franfram/JAGS_LFO/tree/dbab5d599de6bbe2e8a43370a690018279cd557b" target="_blank">dbab5d5</a> </a>
</p>
</div>
<div id="strongRepositoryversionstrongahrefhttpsgithubcomfranframJAGSLFOtreedbab5d599de6bbe2e8a43370a690018279cd557btargetblankdbab5d5a" class="panel-collapse collapse">
<div class="panel-body">
<p>
Great! You are using Git for version control. Tracking code development and connecting the code version to the results is critical for reproducibility.
</p>
<p>
The results in this page were generated with repository version <a href="https://github.com/franfram/JAGS_LFO/tree/dbab5d599de6bbe2e8a43370a690018279cd557b" target="_blank">dbab5d5</a>. See the <em>Past versions</em> tab to see a history of the changes made to the R Markdown and HTML files.
</p>
<p>
Note that you need to be careful to ensure that all relevant files for the analysis have been committed to Git prior to generating the results (you can use <code>wflow_publish</code> or <code>wflow_git_commit</code>). workflowr only checks the R Markdown file, but you know if there are other scripts or data files that it depends on. Below is the status of the Git repository when the results were generated:
</p>
<pre><code>
Ignored files:
    Ignored:    .Rhistory
    Ignored:    .Rproj.user/

Untracked files:
    Untracked:  an autoregresive model with latent variables
    Untracked:  analysis/images/
    Untracked:  analysis/main.Rmd
    Untracked:  analysis/mainbp.Rmd
    Untracked:  analysis/test.Rmd
    Untracked:  analysis/vehtari-paper_notes.Rmd

Unstaged changes:
    Modified:   analysis/info_theory.rmd

</code></pre>
<p>
Note that any generated files, e.g. HTML, png, CSS, etc., are not included in this status report because it is ok for generated content to have uncommitted changes.
</p>
</div>
</div>
</div>
</div>
<hr>
</div>
<div id="versions" class="tab-pane fade">

<p>
These are the previous versions of the repository in which changes were made to the R Markdown (<code>analysis/info_theory.rmd</code>) and HTML (<code>docs/info_theory.html</code>) files. If you’ve configured a remote Git repository (see <code>?wflow_git_remote</code>), click on the hyperlinks in the table below to view the files as they were in that past version.
</p>
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
File
</th>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
<th>
Message
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/franfram/JAGS_LFO/blob/dbab5d599de6bbe2e8a43370a690018279cd557b/analysis/info_theory.rmd" target="_blank">dbab5d5</a>
</td>
<td>
franfram
</td>
<td>
2021-10-19
</td>
<td>
wflow_git_commit("analysis/info_theory.rmd")
</td>
</tr>
</tbody>
</table>
</div>
<hr>
</div>
</div>
</div>
<p>Bugs:</p>
<ul>
<li><p>there may be duplicated parts</p></li>
<li><p>math display bugs: try a different theme and see if it changes</p></li>
</ul>
<p>index:</p>
<ul>
<li><div>
<ol style="list-style-type: decimal">
<li>why are we interested in prediction accuracy? (good paragraph from gelman).</li>
</ol>
</div></li>
<li><div>
<ol start="2" style="list-style-type: decimal">
<li>or 3. idea of predictive distribution vs true but unknown distribution. KL divergence.</li>
</ol>
</div></li>
<li><div>
<ol start="3" style="list-style-type: decimal">
<li>or 2. how do we measure predictive accuracy (general probability scoring rules, then logscores)</li>
</ol>
</div></li>
<li><p>what is IC doing? (good shit from gelman and from lambert).</p></li>
<li><p>what should we do if we are doing LOO-CV. for illustration and to stand the difference with LFO-CV</p></li>
<li><p>what should we do if we are doing LFO-CV.</p></li>
</ul>
<hr />
<p><u><strong>from colah’s blog</strong></u>: <a href="https://colah.github.io/posts/2015-09-Visual-Information/" class="uri">https://colah.github.io/posts/2015-09-Visual-Information/</a></p>
<p>There is simply a fundamental limit. Communicating what word was said, what event from this distribution occurred, requires us to communicate at least 1.75 bits on average. No matter how clever our code, it’s impossible to get the average message length to be less. <strong>We call this fundamental limit the entropy of the distribution</strong> – we’ll discuss it in much more detail shortly.</p>
<p>If we want to understand this limit, the crux of the matter is understanding the trade off between making some codewords short and others long. Once we understand that, we’ll be able to understand what the best possible codes are like.</p>
<p><strong>So, why should you care about cross-entropy? Well, cross-entropy gives us a way to express how different two probability distributions are. The more different the distributions p and q are, the more the cross-entropy of p with respect to q will be bigger than the entropy of p.</strong></p>
<p><strong>Similarly, the more different p is from q, the more the cross-entropy of q with respect to p will be bigger than the entropy of q</strong>.</p>
<p><strong>The really interesting thing is the difference between the entropy and the cross-entropy</strong>. That difference is how much longer our messages are because we used a code optimized for a different distribution. If the distributions are the same, this difference will be zero. As the difference grows, it will get bigger.</p>
<p>We call this difference the Kullback–Leibler divergence, or just the KL divergence. The KL divergence of p with respect to q, Dq(p)</p>
<p>,<sup><a href="http://colah.github.io/posts/2015-09-Visual-Information/#fn5">5</a></sup> is defined:<sup><a href="http://colah.github.io/posts/2015-09-Visual-Information/#fn6">6</a></sup></p>
<p>Dq(p)=Hq(p)−H(p)</p>
<p>If you expand the definition of KL divergence, you get:</p>
<p>Dq(p)=∑xp(x)log2(p(x)q(x))</p>
<p>That might look a bit strange. How should we interpret it? Well, log2(p(x)q(x)) is just the difference between how many bits a code optimized for q and a code optimized for p would use to represent x. The expression as a whole is the expected difference in how many bits the two codes would use <!--# maybe replace "code optimized" for "distribution" or "assumed distribution" and it will make more sense in terms of bayesian data analysis. -->.</p>
<p><strong>The really neat thing about KL divergence is that it’s like a distance between two distributions. It measures how different they are</strong>! (If you take that idea seriously, you end up with information geometry.)</p>
<p><strong>Cross-Entropy and KL divergence are incredibly useful in machine learning. Often, we want one distribution to be close to another. For example, we might want a predicted distribution to be close to the ground truth. KL divergence gives us a natural way to do this, and so it shows up everywhere.</strong></p>
<hr />
<p><u><strong>from aurelion geron</strong></u>: <a href="https://www.youtube.com/watch?v=ErfnhcEV1O8&amp;t=324s" class="uri">https://www.youtube.com/watch?v=ErfnhcEV1O8&amp;t=324s</a></p>
<p>cross entropy is a very comon cost function when training a machine learning classifier.</p>
<p>To transmit one bit of information means to reduce the recipient uncertainty by a factor of 2.</p>
<p>The uncertainty reduction is the inverse of the events probability. If the event A’s probability is 0.5 and somebody told you the outcome is in fact A, then the reduction in uncertainty is 1/0.5 = 2. If the events probability is 0.75, then the uncertainty reduction is 1/0.75 = 4/3.</p>
<p><strong>The entropy H(p) is a measure of how uncertain the events are. It measures the average amount of information that you get from one sample drawn from a given probability distribution P. It tells you how unpredictable that probability distribution is.</strong></p>
<p><span class="math inline">\(H(\pmb{p})-\sum_i \, p_i \,\, log_2(p_i)\)</span></p>
<p>NOTE: If there are 8 possible states of an event and all are equally likely, and then somebody tells you which state is going to happen, then they reduced your uncertainty by a factor of 8. This means they gave you 3 bits of information (<span class="math inline">\(2^3 = 8\)</span>). The number of bits of information <span class="math inline">\((3)\)</span> that were actually communicated can be found computing the binary logarithm <span class="math inline">\(log_2 ()\)</span> of the <strong>uncertainty reduction factor</strong> <span class="math inline">\((8)\)</span>, thus <span class="math inline">\(log_2 (8) = 3\)</span>. In a more general way, where there are many posible states of an event and where the probabilities of each of the states aren’t equal, the uncertainty reduction factor is just the inverse of the probability of a given state: if some state A has a probability of 0.25 and you somebody tells you that the state A will in fact happend, now the uncertainty reduction factor is 1/0.25 and thus the amount of information provided is <span class="math inline">\(log_2(1/0.25) = 2\)</span> bits. Now, the <span class="math inline">\(log(1/x) = - \, log(x)\)</span>, so the equation to compute the number of bits simplifies to <span class="math inline">\(- \, log_2(0.25) = 2\)</span>. The information that you are going to get, on average, is equal to the probability of each state times the amount of information it provides, thus: <span class="math inline">\(-\sum_i \, p_i \, log_2(p_i)\)</span>, which is in fact the entropy <span class="math inline">\(H(\pmb{p})\)</span></p>
<p>Cross entropy is the average message length.</p>
<p>Cross entropy can be expressed as a function of both the true probability distribution <strong>p</strong> and the predicted probability distribution <strong>q</strong></p>
<p>Cross entropy: <span class="math inline">\(H(\pmb{p, q})-\sum_i \, p_i \,\, log_2(q_i)\)</span></p>
<p><strong>The cross entropy equation looks very much like the entropy equation, but instead of computing the log of the true probability, we use the log of the predicted probability, which is equal to the message length. If our predictions are perfect, thats is the predicted distribution is equal to the true distribution, then the cross-entropy is simply equal to the entropy</strong>. But if the distributions differ, then the cross-entropy will be greater than the entropy by some number of bits (m’ so i guess its somehow related to the specific “encoding”, but what is this encoding mean? the distribution you think thats true? that is, the predicted distribution?. ). <strong>This amount by which the cross-entropy exceeds the entropy is called the relative entropy, or more commonly the Kullback-Leibler Divergence (or KL Divergence)</strong>. Thus:</p>
<p>Cross-entropy = Entropy + KL Divergence</p>
<p>Or, equivalently, the KL Divergence (noted <span class="math inline">\(D_{KL} (\pmb{p} || \pmb{q})\)</span>) is equal to the cross-entropy <span class="math inline">\(H(\pmb{p, q})\)</span> minus the entropy <span class="math inline">\(H(\pmb{p})\)</span>.</p>
<p><span class="math inline">\(D_{KL} (\pmb{p} || \pmb{q}) = H(\pmb{p, q}) - H(\pmb{p})\)</span></p>
<p>NEED TO CHECK THE FINAL EXAMPLE</p>
<hr />
<p>from Adian Liusie <a href="https://www.youtube.com/watch?v=0GCGaw0QOhA" class="uri">https://www.youtube.com/watch?v=0GCGaw0QOhA</a></p>
<p>Shannon Entropy measures the uncertainty of a probability distribution.</p>
<p>more uncertainty, more entropy</p>
<hr />
<p><u><strong>from info theory book</strong></u>:</p>
<p>logs turn multiplication into sums. m’ and thats much stable due to? numerical over and underflow?</p>
<hr />
<p><u><strong>from mathmatical monk on logsumexp</strong></u> <a href="https://www.youtube.com/watch?v=-RVM21Voo7Q&amp;t=2s" class="uri">https://www.youtube.com/watch?v=-RVM21Voo7Q&amp;t=2s</a></p>
<p>underflow: when you are trying to represent a number on a computer and the number is to small to represent. When you get a super small numer, it automatically sets to 0 in some programming languages <!--# this is what happens in R -->.</p>
<p><strong>underflow can be fixed with the log-sum-exp trick</strong>.</p>
<p>underflow tends to happen when you multiply many probabilities together or when you many data and compute the probability of a given event.</p>
<p>When you want to represent a very small number you can use logs.</p>
<p>If u have Z = P(A) * P(B) * P(C), then</p>
<p>Z = exp(logP(A) + logP(B) + logP(C)), but this way Z will still be very small, so we need log(Z), then</p>
<p>log(Z) = log (exp(logP(A) + logP(B) + logP(C)))</p>
<p>If we have a sum, such as <span class="math inline">\(\sum_{i=1}^N P(A|Bi) * P(Bi) = Z\)</span>, then</p>
<p>Z = sum( exp( logP(A|Bi) + logP(Bi) ) ), and</p>
<p>log(Z) = log( sum( exp( logP(A|Bi) + logP(Bi) ) ) )</p>
<p>but theres a problem because if for example P(Bi) is small, then logP(Bi) is going to be a negative but big in module number, and taking the exp of that will end up underflowing to 0. So a fix to that is…</p>
<p>TO BE CONTINUED (LEFT AT MIN 6:40)</p>
<hr />
<p><u><strong>from luis serrano’s vid</strong></u>: <a href="https://www.youtube.com/watch?v=9r7FIXEAGvs" class="uri">https://www.youtube.com/watch?v=9r7FIXEAGvs</a></p>
<p>products are small and volatile, sums are good</p>
<p><strong>log is a function that turns products into sums due to the following identity</strong>: <span class="math inline">\(log(a*b) = log(a) + log(b)\)</span>.</p>
<hr />
<p>from <a href="https://www.youtube.com/watch?v=tRsSi_sqXjI" class="uri">https://www.youtube.com/watch?v=tRsSi_sqXjI</a></p>
<p>cross entropy is a way to measure the distance between two probability vectors (m’ also between distributions?). <strong>Cross entropy is not symmetric</strong></p>
<hr />
<p><u><strong>from ben lambert’s KL divergence video</strong></u> <a href="https://www.youtube.com/watch?v=LJwtEaP2xKA&amp;list=PLwJRxp3blEvZ8AKMXOy0fc0cqT61GsKCG&amp;index=34" class="uri">https://www.youtube.com/watch?v=LJwtEaP2xKA&amp;list=PLwJRxp3blEvZ8AKMXOy0fc0cqT61GsKCG&amp;index=34</a>:</p>
<p>So the kullback liebler divergence us exactly the difference in the lenth or the expected length of code messages written in the two different encodings for a language written in P. So the KL divergence provides a kind of measure of informational cost. The informational cost that it measures is the increased length of messages from using a sub optimal encoding of a given language. The KL divergence kind of provides a lower bound on informational cost.</p>
<hr />
<p>from ben lambert’s Ideal measure of a model’s predictive fit. <a href="https://www.youtube.com/watch?v=MEqrDu-ytM8&amp;list=PLwJRxp3blEvZ8AKMXOy0fc0cqT61GsKCG&amp;index=35" class="uri">https://www.youtube.com/watch?v=MEqrDu-ytM8&amp;list=PLwJRxp3blEvZ8AKMXOy0fc0cqT61GsKCG&amp;index=35</a>.</p>
<p>Given a posterior distribution <span class="math inline">\(p(\theta | \pmb{x})\)</span>, we can produce a posterior predictive distribution, which for a single new data point equals <span class="math inline">\(p(x^{new} | \pmb{x})\)</span> (more detail on this on here NOTE), and this just tells us what is our sort of best prediction or distribution which ecompasses what we think about predictions for the next data point given that we have observed a data vector <span class="math inline">\(\pmb{x}\)</span>. This distribution <span class="math inline">\(p(x^{new} | \pmb{x})\)</span> is an approximation to some unkown true data generating process which, we can call <span class="math inline">\(f(x^{new})\)</span>. Ideally, if we knew what the distribution <span class="math inline">\(f(x^{new})\)</span> was, we could compare it with our posterior predictive distribution <span class="math inline">\(p(x^{new} | \pmb{x})\)</span>. The best way we could compare this two distributions (if we knew both of them), is with what is called the Kullback-Leibler divergence (<span class="math inline">\(D_{KL}\)</span>, also known as “relative entropy”), which is just a general measure of the difference between two distributions.</p>
<!--# from wiki: The KL divergence is a measure of how one probability distribution is different from a second, reference probability distribution. In the simplified case, a relative entropy of 0 indicates that the two distributions in question have identical quantities of information. [...] Consider two probability distributions P and Q. Usually, P represents the data, the observations, or a measures probability distribution. Distribution Q represents instead a theory, a model, a description or an approximation of P. The KL divergence is then interpreted as the average difference of the number of bits required for encoding samples of P using a code optimized for Q rather than one optimized for P (('m im starting to think that "code" is equivalent to "distribution" in statistics, because codes that are optimized always imply a certain distribution, since the optimization aims to use shorter codes for very likely outcomes')). [...] Various conventions exist for referreing to $D_{KL}(P||Q)$ in words. Often it is referred to as the divergence between P and Q, but this fails to convey the fundamental asymmetry in the relation. Sometimes, as in this article, it may be described as the divergence of P from Q or as the divergence from Q to P. This reflects the asymmetry in Bayesian inference, which starts from a prior Q and updates to the posterior P. Another common way to refer to $D_{KL}(P||Q)$ is as the relative entropy of P with respect to Q.  -->
<p>The Kullback-Leibler divergence in going from <span class="math inline">\(f(x^{new})\)</span> (our true distribution) to our posterior predictive distribution <span class="math inline">\(P(x^{new} | \pmb{x})\)</span> is equal to:</p>
<p><span class="math display">\[
D_{KL}(f(x^{new}) \rightarrow p(x^{new} | \pmb{x} )) = \int f(x^{new})\ log\ f(x^{new})dx^{new}\ -\ \int f(x^{new})\ log\ p(x^{new}| \pmb x)\ dx^{new} 
\]</span></p>
<p>The first term, <span class="math inline">\(\int f(x^{new})\ log \ f(x^{new})dx^{new}\)</span> <!--# isnt there a minus sign missing at the beggining? or is it that the second term "should" come first and there's a minus minus sign (thus plus sign) to the first term? maybe its related to the mathmatical definition of cross-entropy and its relation with entropy and KL divergence. -->, is essentially a constant because it only depends on <span class="math inline">\(f(x^{new})\)</span> and so that doesn’t tell us anything about how well our model fits the data (and there’s nothing we can do about that term). Whereas the second term, <span class="math inline">\(\int f(x^{new})\ log\ p(x^{new}| \pmb x)\ dx^{new}\)</span> , contains <span class="math inline">\(f(x^{new})\)</span> (our unknown true data generation process) but also the posterior predictive distribution <span class="math inline">\(p(x^{new} | \pmb{x})\)</span> which depends on the model that we fit (so there’s something we can do about this term). This second term is called the “Expected Log Predictive Density” (ELPD) <!--# "expected" cuz we are integrating with respect to xnew? check gelman -->. We want to minimize the KL divergence because when <span class="math inline">\(D_{KL} = 0\)</span>, then <span class="math inline">\(f(x^{new}) = p(x^{new} | \pmb{x})\)</span> (that is, our approximation <span class="math inline">\(p(x^{new} | \pmb{x})\)</span> of the unknown true data generation process <span class="math inline">\(f(x^{new})\)</span> is correct). And the only way we can minimize the KL divergence is by maximizing the ELPD <!--# maximizing because of the minus sign -->.</p>
<!--# maybe explain the KL divergence saying what each term is, i.e., this term is the entropy and this other term is the cross entropy.  -->
<p>The reason why the second term is called the Expected Log Predictive Density (ELPD) is that, since we are integrating with respect to <span class="math inline">\(x^{new}\)</span>, what we obtain is the expected value <span class="math inline">\(E\)</span> under the true data generating process <span class="math inline">\(f\)</span> (thus, <span class="math inline">\(E_f\)</span>) of the log of the predictive density <span class="math inline">\(log \ p(x^{new} | \pmb{x})\)</span> (the posterior predictive distribution is also called “predictive density”).</p>
<p><span class="math display">\[
ELPD = E_f\  [ log\ p(x^{new} | \pmb{x}) ] 
\]</span></p>
<p>Is important to note that the ELPD is an ideal measure of a models fit to data, because it assumes that we actually know <span class="math inline">\(f\)</span>, whereas in practice we never know it. So what we do instead is try to estimate it, so what we get is <span class="math inline">\(\widehat{ELPD}\)</span>.</p>
<hr />
<p>from ben lambert’s vid on Evaluating model fit through AIC, DIC, WAIC and LOO-CV: <a href="https://www.youtube.com/watch?v=xS4jDHQfP2o&amp;list=PLwJRxp3blEvZ8AKMXOy0fc0cqT61GsKCG&amp;index=60" class="uri">https://www.youtube.com/watch?v=xS4jDHQfP2o&amp;list=PLwJRxp3blEvZ8AKMXOy0fc0cqT61GsKCG&amp;index=60</a></p>
<p>In this video im going to talk through some of the most common metrics for evaluating the predictive fit of your model to data, specificly AIC, DIC, WAIC and LOO-CV. So that the idea which is common to all of these criteria is that we have a sample of data and we use that sample of data to fit our model. However, what we would like to do is we would like to know how well our model generalizes to out of sample prediction. So, ideally, what we would do is we would fit our model using one sample of data and then evaluate its predictive fit on another set of data. However, in real life data can be hard to come by and so we dont necessarily have an independent data set on which to evaluate the fit of our model, so often what we’re forced to do is to evaluate the fit of our model to data using the same sample that we used to fit our model in the first place. So we are trying to determine how well our model generalizes to a new sample of data but using the same sample of data and not a new one, which brings a few problems. One of the main problems is selection bias <!--# need to know what this actually is --> , because of the fact that we are using sample of data to fit then test our model, we are going to have an issue with getting an overinflated sense of how well the model can predict and the sample data and we run into the issue of having an over fit model, which means that the model is fitting the noise in a data sample rather than only the signal. The cause of this overfitting is the build of a model which is overly complex for a given circumstance, and this overfit model wont actually generalize well to new samples of data. So all information criteria (which are the ones that use in sample data to predict out of sample accuracy) have to apply a kind of correction to correct for the fact that there is this selection bias going on. So all of this information criteria come out with some measure of out of sample predictive accuracy but each of them goes about obtaining a measure of a design for.</p>
<hr />
<p>MUST CHECK VEHTARIS CV FOR HIERARCHICAL MODELS BLOG POST: <a href="https://avehtari.github.io/modelselection/rats_kcv.html" class="uri">https://avehtari.github.io/modelselection/rats_kcv.html</a></p>
<hr />
<p>from be lamberts vid con estimating the posterior predictive distribution by sampling: <a href="https://www.youtube.com/watch?v=TMnXQ6G6E5Y" class="uri">https://www.youtube.com/watch?v=TMnXQ6G6E5Y</a></p>
<p>theres an exact equation for the posterior predictive distribution but for most applied cases this equation requires intractable calculations to be done, so be cant obtain the exact posterior predictive distribution. Instead, we approximate it by sampling. The posterior predictive distribution is the probability distribution over some new data <span class="math inline">\(\tilde{X}\ or\ X^{new}\)</span> given that we have observed our sample <span class="math inline">\(X\)</span>, that is <span class="math inline">\(p(\tilde{X} | X)\)</span>.</p>
<hr />
<p><strong>FROM BEN LAMBERTS BOOK.</strong></p>
<p>7.8 From Posterior to Predictions by Sampling.</p>
<p>There are two sources of uncertainty in prediction: first, we do not know the true value of the parameters; and second, there is sampling variability <!--# i guess he means data sampling variability -->. The first of these sources of uncertainty is represented by the posterior. The second is represented by our choice of likelihood. To account for both these sources of variation, we typically derive an approximate distribution that represents our uncertainty over future data by iterating the following steps:</p>
<ol style="list-style-type: decimal">
<li><p>sample a parameter from the <strong>posterior</strong> distribution</p>
<p><span class="math display">\[
\theta_i \sim p(\theta | data)
\]</span></p></li>
<li><p>plug that value of <span class="math inline">\(\theta_i\)</span> into our sampling distribution (likelihood) and then sample a datapoint.</p></li>
</ol>
<p><span class="math display">\[
data^{&#39;}_i \sim p(data | \theta_i) 
\]</span></p>
<!--# this comes from the vid: Then if we draw a histogram of all the sampled $data^{'}_{i}$ , we get an approximation to our posterior predictive distribution. -->
<p>By repeating these steps a large number of times (keeping each sampled data value), we eventually obtain a reasonable approximation to the posterior predictive distribution. This distribution represents our uncertainty over the outcome of a future data collection effort, accounting for our observed data and model choice. <!--# have to see if jags log_likelihood function actually does this --></p>
<p>10.5.1 Out-of-sample prediction and overfitting</p>
<p>We build statistical models to explain the variation in a data sample because we believe that the insight this gives us applies to wider circumstances. One way to measure the wider applicability of a statistical model is to evaluate its predictive power on out-of-sample data. By fitting our model to one sample of data and then using it to predict data in another, we hope to produce an unbiased measure of a model’s capacity to generalise. The problem is that we generally do not have access to out-of-sample data. (If we did, we would usually include this as part of our sample!) One way out of this issue is to use the same sample twice: once to fit the statistical model and again to test its predictive power. The trouble with this approach is that we bias the odds in our favour and leave ourselves susceptible to overfitting.</p>
<p>If we try hard enough we can build a model that predicts a given data set perfectly, by adding layer upon layer of additional complexity. In producing a more complex model, we make more extreme assumptions about the data-generating process that may or, more likely, may not, be true. When we obtain a fresh data set, these assumptions are tested and often found wanting <!--# is "wanting" really the word? or is  -->, meaning that the model is terrible for prediction.</p>
<p>In machine learning (a statistical framework for developing the most predictive models), overfitting is avoided by splitting a data set into a training set and a cross-validation set. The models are fitted to the training set, which is then assessed by its performance on an independent cross- validation set.</p>
<!--# why IC has been used more often than CV -->
<p>While we would ideally carry out this sort of partitioning in statistical inference (in fact, one measure we discuss, LOO-CV, does exactly this), often the computational task of refitting a model on many data sets makes it prohibitive to do so. <strong>Also, the nature of the data can make it difficult to decide on an appropriate data split</strong>. Among other reasons, these two issues have led to a demand for other measures of model predictive capability, which can be calculated without the need for re-estimation on a cross-validation set.</p>
<p>These measures are, at best, approximations to the cross-validation ideal <span class="math display">\[15\]</span>. They aim to correct for the bias inherent in trying to assess a model’s performance on the same data set which was used to fit the model in the first place.</p>
<!--# notice that earlier Ben asseses the problem with complex models, mainly that they assume too many things about the data generating process (if each thing assumed has a probability of being true, then the more things we assume are happening, the less likely it is), and this tends to make the model fit the "noise" (random jitter not inherent to the true generating process or "signal") instead of only the signal (i.e., the model overfits) -->
<p>If we obtain new data <span class="math inline">\(\pmb{y^{new}} = \{y^{new}_1 , \ ..., \ y^{new}_n \}\)</span> there are a few different ways we might measure the fit of a model. A popular way of summarising the discrepancy between the model’s predictions <span class="math inline">\(\pmb{y^{pred}} = \{y^{pred}_1 ,\ ..., \ y^{pred}_n \}\)</span> and the real data <span class="math inline">\(\pmb{y^{new}}\)</span> is to measure the mean squared error (MSE) (maybe add the equation if you are not that lazy).</p>
<p>This measure is easy to calculate but does not have any theoretical justification (apart from when a normal likelihood is used), which limits its scope. A more theoretically justified Bayesian measure would be to use the posterior predictive distribution to measure a model’s ability to predict new data. In particular, we could choose a model with the highest posterior probability of generating the new data, <span class="math inline">\(p(\pmb{y^{new} \ | \ y ) }\)</span>.</p>
<p>Since the log function is a monotonic transformation, the score obtained by using the logarithm of the posterior predictive distribution will mirror the posterior predictive distribution. We favour using the log form because of its connection with a concept called the Kullback–Leibler (KL) divergence, which is a measure of the difference between the true density and the estimated one. If we maximise the log of the posterior predictive distribution, this is equivalent to estimating the posterior predictive density with the lowest KL divergence from the true density.</p>
<p>The scoring function hence used is:</p>
<p><span class="math display">\[
prediction \ accuracy = log[p(\pmb{y^{new} | \ y})]
\]</span></p>
<p>which can also be written as:</p>
<p><span class="math display">\[
prediction \ accuracy = log \ \int p(\pmb{y^{new}} | \theta) \ p(\theta | \pmb{y}) d\theta  \ \ (10.4) 
\]</span></p>
<p><span class="math display">\[
prediction\ accuracy = log[E_{posterior}(p(\pmb{y^{new}}\  |\ \theta  ))] 
\]</span></p>
<p>where <span class="math inline">\(E_{posterior}\)</span> denotes the expectation with respect to the posterior distribution.</p>
<p>10.5.3 The ideal measure of a model’s predictive accuracy.</p>
<p>Imagine that we start by considering a single new data point, <span class="math inline">\(y^{new}\)</span>. Usually, we do not have access to this extra data, and so the new data <span class="math inline">\(y^{new}\)</span> is unknown. If we knew the true distribution <span class="math inline">\(f(y)\)</span> for a single new data point, we could evaluate the expectation of the expression (10.4), which gelman et al <span class="math display">\[14\]</span> call the expected log predictive density (elpd):</p>
<p><span class="math display">\[
elpd = E_f(log[p(y^{new} | \pmb{y})])
\]</span></p>
<p><span class="math display">\[
elpd = \int_{y^{new}\ \epsilon\ Y} f(y^{new})log[p(y^{new} | \pmb{y})]dy^{new} \ \ (10.6)
\]</span></p>
<p>where <span class="math inline">\(E_f\)</span> denotes the expectation under the true data distribution <span class="math inline">\(f(y)\)</span>. This measure quantifies how close the estimated posterior predictive distribution, <span class="math inline">\(p(y^{new} | \pmb{y})\)</span>, is to the true distribution, <span class="math inline">\(f(y)\)</span>. Accordingly, expression (10.6) is maximised when the estimated distribution equals the true one (because in that case the <span class="math inline">\(D_{KL} = 0\)</span> and thus both distributions are the same).</p>
<p>If we choose a model to maximise <span class="math inline">\(elpd\)</span>, we also minimise the KL divergence between our posterior predictive distribution and the true data distribution.</p>
<p>If we then consider the <span class="math inline">\(elpd\)</span> for our <span class="math inline">\(n\)</span> new data points, taken one at a time, we have what Gelman et al. <span class="math display">\[14\]</span> call the “expected log <strong>pointwise</strong> predictive density” (elppd):</p>
<p><span class="math display">\[
elppd = \sum_{i = 1}^{n}E_f( log[ p(y^{new} | \pmb{y} ])     (10.10)
\]</span></p>
<p>The pointwise measure defined in (10.10) is preferable to using the full joint predictive distribution <span class="math inline">\(E_f(\ log\ [p(\pmb{y^{new}}\ |\ \pmb{y})]\ )\)</span> because it enables a range of expressions representing out of sample error to be calculated from it <span class="math display">\[14\]</span></p>
<!--# shouldnt there be an average? or is just the straight sum? -->
<p>… So, in general, we can think about the KL divergence as measuring some sort of informational penalty in going from something optimised to distribution p to code for another distribution q.</p>
<p>10.5.8 LOO-CV</p>
<p>As we discussed previously, the ideal measure of a model’s predictive accuracy would be to split a data set into a training set and a cross-validation set. The model is then fitted on the training set, and its predictive performance gauged on the independent cross-validation set. The use of this independent cross-validation set <strong>circumvents the issue of selection bias</strong>, and allows us to be more confident in our estimates of the model’s out-of-sample predictive capability.</p>
<p>While the use of training and cross-validation sets provides a better measure of predictive accuracy in principle, there are practical concerns which limits its use. Here, we consider a method known as leave-one-out cross-validation (LOO-CV), where we use a single data point to test the model’s predictive power, <span class="math inline">\(y_{cv}\)</span>, and use the rest of the sample, <span class="math inline">\(\pmb{y_{train}}\)</span>, to train the model. Ideally, this process is iterated <span class="math inline">\(n\)</span> times (where <span class="math inline">\(n\)</span> is the size of the data sample) so that each data point in the sample is used only once as the single cross-validation test datum in one of the iterations. This can be extremely expensive computationally speaking, particularly if the data set is large. Also, if the data are structured, as in te case of time series, it may be difficult to estimate the model with gaps in the data series (with the “gap” being the left out datum for testing).</p>
<p>In each iteration of LOO-CV we evaluate the log posterior predictive density (across all samples from our posterior distribution):</p>
<p><span class="math display">\[
lpd = log[p(y_i|\pmb{y_{-i}})]
\]</span></p>
<p>where <span class="math inline">\(\pmb{y_{i-1}}\)</span> denotes the training data vector with all data points included apart from <span class="math inline">\(y_i\)</span> (the bold letter indicates that it’s a vector instead of a scalar). If this process is iterated <span class="math inline">\(n\)</span> times, we can estimate the overall expected log pointwise predictive density by summing the invidiual <span class="math inline">\(lpd\)</span> values. Thus:</p>
<p><span class="math display">\[
\widehat{elppd} = \sum_{i = 1}^{n}\ log[p(y_i|\pmb{y_{-i}})]
\]</span></p>
<p>Estimates of <span class="math inline">\(elppd\)</span> by this method may underestimate the predictive accuracy of the full model because the training sample consists of only <span class="math inline">\(n - 1\)</span> data points rather than the full sample <span class="math display">\[14\]</span>. So a corrective term can be added for completeness; however, in paractice this term is small (particularly for large data sets), so it can usually be ignored. <!--# CHECK GELMANS PAPER BECAUSE WE MAY HAVE TO ADD THE CORRECTION TERM BECAUSE WE DONT HAVE THAT MANY DATA --></p>
<p>In using LOO-CV to choose between models, we also run the risk of overfitting. This is because, if we use LOO-CV to select a model, we will likely pick one that fits both the signal and noise in the cross-validation set. <!--# this is not really an explanation --> This is well documented in the machine-learning literature and merits the use of a third data set called the <span class="math inline">\(test\)</span> set, which is used only once to evaluate a model.</p>
<p><strong>10.5.9</strong> A practical summary of measures of predictive accuracy in simple terms.</p>
<p>We have now introduced the theoretical ideal measure of a model’s predictive accuracy and the various methods used to approximate this. Ultimately, we would like a measure that is Bayesian as well as a reasonable approximation for the out-of-sample predictive accuracy of a given model. While AIC and DIC are commonly used, they are not fully Bayesian in nature. WAIC is fully Bayesian as well as being closer to the ideal measure of a model’s out-of-sample predictive accuracy. However, this method, like AIC and DIC, estimates out-of-sample predictive accuracy from within the same sample that was used to originally fit the model, meaning that post-hoc <!--# Post hoc (a shortened form of post hoc, ergo propter hoc) is a logical fallacy in which one event is said to be the cause of a later event simply because it occurred earlier. --> bias corrections are required to correct for overfitting. LOO-CV partitions the sample into a training set, which is used to fit the model, and a single cross-validation data point, which is used to estimate out-of-sample predictive accuracy. Since this method uses an independent data set to assess the predictive accuracy, it avoids the need to correct for the overfitting bias inherent in the other methods. In this respect, this method is the closest, and cleanest, approximation to the ideal measure of out-of-sample predictive accuracy. However, there is a penalty to LOO-CV, in that it requires repeated estimation on each of the <span class="math inline">\(n\)</span> training and cross-validation set pairs, which may be computationally infeasible for complex models. Also, both LOO-CV and WAIC require a partitioning of the data sample into subsamples, which may not be straightforward for situations where the data are structured (for example, in time series, panel or network data). AIC and DIC do not require such a partitioning and hence are more amenable in these circumstances.</p>
<p>For a more thorough perspective on measures of model predictive accuracy, see <span class="math display">\[14\]</span> and <span class="math display">\[15\]</span>, which served as invaluable references for this section.</p>
<p>10.9 Chapter summary</p>
<p>The reader should now understand how to critically assess a Bayesian model and choose between different models. Posterior predictive checks (PPCs) are used to compare the fit of a given model to some aspect of the sample data, and statistical measures like WAIC and LOO-CV are used to compare models in terms of their out-of-sample predictive accuracy. You may be wondering why we need both of these approaches. This is because the purpose of these two frameworks is different, although complementary. PPCs assess the fit of your model to the data at hand, whereas WAIC and LOO-CV assess the fit of your model to out-of-sample data. The trouble with blindly using PPCs to construct a model is that this can result in an overly complex model, which is overfit to the sample of data on which it was estimated. We guard against this overfitting by using WAIC and LOO-CV. In reality, these two different frameworks should be used in tandem throughout the modelling process; we would not want to choose a model with a good predictive accuracy but failed to represent some key aspect of variation seen in the data. It can also be useful to combine aspects of PPCs with measures of predictive accuracy. For example, we could partition our data set into a training and cross-validation set, and see how a model fitted to the former performs on PPCs on the latter.</p>
<hr />
<p><u><strong>FROM “Understanding predictive information criteria for Bayesian models” GELMAN ET AL 2013.</strong></u></p>
<p>We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where <strong>the goal is to estimate expected out-of-sample-prediction error using a bias-corrected adjustment of within-sample error</strong>.</p>
<p>In other settings, however, we seek not to check models but to compare them and explore directions for improvement. Even if all of the models being considered have mismatches with the data, it can be informative to evaluate their predictive accuracy, compare them, and consider where to go next. <strong>The challenge then is to estimate predictive model accuracy, correcting for the bias inherent in evaluating a model’s predictions of the data that were used to fit it.</strong> <!--# THIS THE CHALLENGE TO IC --></p>
<p>A natural way to estimate out-of-sample prediction error is cross-validation (see Geisser and Eddy, 1979, and Vehtari and Lampinen, 2002, for a Bayesian perspective), but researchers have always sought alternative measures, as cross-validation requires repeated model fits and can run into trouble with sparse data. For practical reasons alone, there remains a place for simple bias corrections such as AIC (Akaike, 1973), DIC (Spiegelhalter et al., 2002, van der Linde, 2005), and, more recently, WAIC (Watanabe, 2010), and all these can be viewed as approximations to different versions of cross-validation (Stone, 1977).</p>
<p>Various difficulties have been noted with DIC (see Celeux et al., 2006, Plummer, 2008, and much of the discussion of Spiegelhalter et al., 2002) but there has been no consensus on an alternative.</p>
<p><strong>One difficulty is that all the proposed measures are attempting to perform what is, in general, an impossible task: to obtain an unbiased (or approximately unbiased) and accurate measure of out-of-sample prediction error that will be valid over a general class of models and that requires minimal computation beyond that needed to fit the model in the first place</strong>. When framed this way, it should be no surprise to learn that no such ideal method exists. But we fear that the lack of this panacea has impeded practical advances, in that applied users are left with a bewildering array of choices.</p>
<p>In some ways, our paper is similar to the review article by Gelfand and Dey (1994), except that they were focused on model choice whereas our goal is more immediately to <strong>estimate predictive accuracy for the goal of model comparison</strong>.</p>
<p>As we shall discuss in the context of an example, given the choice between two particular models, <strong>we might prefer the one with higher expected predictive error</strong> (WTF?) ; nonetheless we see predictive accuracy as one of the criteria that can be used to evaluate, understand, and compare models.</p>
<p>2. Log predictive density as a measure of model accuracy</p>
<p><strong>One way to evaluate a model is through the accuracy of its predictions. Sometimes we care about this accuracy for its own sake, as when evaluating a forecast. In other settings, predictive accuracy is valued not for its own sake but rather for comparing different models.</strong> We begin by considering different ways of defining the accuracy or error of a model’s predictions, then discuss methods for estimating predictive accuracy or error from data.</p>
<p>2.1 Measures of predictive accuracy.</p>
<p>Consider data <span class="math inline">\(y_1,\ …,\ y_n\)</span>, modeled as independent given parameters <span class="math inline">\(\theta\)</span>, thus <span class="math inline">\(p(y|\theta) = \prod_{i = 0}^{n}\ p(y_i | \theta)\)</span>. With regression, one would work with <span class="math inline">\(p(y|\theta, x) = \prod_{i = 0}^{n}\ p(y_i | \theta,\ x_i)\)</span>. In our notation, we suppress any dependence on <span class="math inline">\(x\)</span>.</p>
<p>Preferably, the measure of predictive accuracy is specifically tailored for the application at hand, and it measures as correctly as possible the benefit (or cost) of predicting future data with the model. Often explicit benefit or cost information is not available and the predictive performance of a model is assessed by generic scoring functions and rules.</p>
<p><strong>Measures of predictive accuracy for probabilistic prediction are called scoring rules</strong> <!--# maybe just say whats behind the comment and add that there are other scoring rules besided the logarithmic one -->. Examples include the quadratic, logarithmic, and zero-one scores, whose properties are reviewed by Gneiting and Raftery (2007). Bernardo and Smith (1994) argue that suitable scoring rules for prediction are proper and local: propriety of the scoring rule motivates the decision maker to report his or her beliefs honestly, and for local scoring rules predictions are judged only on the plausibility they assign to the event that was actually observed, not on predictions of other events. The logarithmic score is the unique (up to an affine transformation) <!--# In Euclidean geometry, an affine transformation, or an affinity, is a geometric transformation that preserves lines and parallelism (but not necessarily distances and angles) --> local and proper scoring rule (Bernardo, 1979), and appears to be the most commonly used scoring rule in model selection.</p>
<p><strong>Mean squared error</strong>. A model’s fit to new data can be summarized numerically by mean squared error, <span class="math inline">\(\frac{1}{n}\ \sum_{i = 1}^{n}(y_i - E(y_i |\theta))^2\)</span> , or a weighted version such as <span class="math inline">\(\frac{1}{n}\ \sum_{i = 1}^{n}(y_i - E(y_i |\theta))^2 /var(y_i | \theta)\)</span>. These measures have the advantage of being easy to compute and, more importantly, to interpret, but the disadvantage of being less appropriate for models that are far from the normal distribution.</p>
<p><strong>Log predictive density or log-likelihood</strong>. Log predictive density or log-likelihood. A more general summary of predictive fit is the log predictive density, <span class="math inline">\(log\ p(y|\theta)\)</span> <!--# this is different from the log predictive density of ben lambert's book log p(ynew | \pmb{y}) -->, which is proportional to the mean squared error if the model is normal with constant variance. <u><strong>The log predictive density is also sometimes called the log-likelihood</strong></u> <!--# SUPERRRR IMPORTNAT, THIS WHY I DIDNT REALLY GET THE CODE -->. <strong>The log predictive density has an important role in statistical model comparison because of its connection to the Kullback-Leibler information measure</strong> (see Burnham and Anderson, 2002, and Robert, 1996). <strong>In the limit of large sample sizes, the model with the lowest Kullback-Leibler information—and thus, the highest expected log predictive density—will have the highest posterior probability. Thus, it seems reasonable to use expected log predictive density as a measure of overall model fit</strong>.</p>
<p>Given that we are working with the log predictive density, the question may arise: why not use the log posterior? Why only use the data model and not the prior density in this calculation? The answer is that we are interested here in summarizing the fit of model to data, and for this purpose he prior is relevant in estimating the parameters but not in assessing a model’s accuracy.</p>
<p>We are not saying that the prior cannot be used in assessing a model’s fit to data <!--# with "prior" does he mean the "the new posterior" and thus by "likelihood" he means the posterior predictive density? which would be some sort of "posterior" likelihood, the likelihood after fitting the model, that is, the likelihood with the posterior parameters instead of the prior parameters. -->; <strong>rather we say that the prior density is not relevant in computing predictive accuracy</strong>. <strong>Predictive accuracy is not the only concern when evaluating a model, and even within the bailiwick</strong> <!--# Bailiwick nowadays means "one's area of skill, knowledge, authority, or work" --> <strong>of predictive accuracy, the prior is relevant in that it affects inferences about</strong> <span class="math inline">\(\theta\)</span> and thus af<strong>fects any calculations involving</strong> <span class="math inline">\(p(y|\theta)\)</span>. In a sparse-data setting, a poor choice of prior distribution can lead to weak inferences and poor predictions.</p>
<p>2.3. Predictive accuracy for a single data point.</p>
<p><strong>The ideal measure of a model’s fit would be its out-of-sample predictive performance for new data produced from the true data-generating process</strong>. We label <span class="math inline">\(f\)</span> as the true model, <span class="math inline">\(y\)</span> as the observed data (thus, a single realization of the dataset <span class="math inline">\(y\)</span> from the distribution <span class="math inline">\(f(y)\)</span>, and <span class="math inline">\(\tilde{y}\)</span> as future data or alternative datasets that could have been seen. The out-of-sample predictive <strong>fit</strong> for a <strong>new</strong> data point <span class="math inline">\(\tilde{y_i}\)</span> <strong>using logarithmic score</strong> <!--# NOTE THAT WE ARE USING LOGARITHMIC SCORE, SO NOW U CAN SAY SOMETHING LIKE "THERE ARE OTHER'S SCORING RULES BESIDES THE LOGARITHMIC SCORE" --> is then,</p>
<p><span class="math display">\[
log\ p_{post}(\tilde{y_i}) = log\ E_{post} (p(\tilde{y_i} | \theta) = log\ \int p(\tilde{y_i} | \theta)\ p_{post}(\theta)d \theta
\]</span></p>
<p>In the above expression, <span class="math inline">\(p_{post}(\tilde{y_i})\)</span> is the predictive density <!--# or posterior predictive density, which i think is the probability of generating ytilde with the given model, or in other way, how likely is this ytilde for the given model  --> for <span class="math inline">\(\tilde{y_i}\)</span> induced by the posterior distribution <span class="math inline">\(p_{post}(\theta)\)</span>. We have introduced the notation <span class="math inline">\(p_{post}\)</span> here to represent the posterior distribution because our expressions will soon become more complicated and it will be convenient to avoid explicitly showing the conditioning. <!--# E_{post} denotes the expectation under the posterior distribution, that is the \int p_{post}\theta d\theta part. --></p>
<p>We must then take on further step. The future data <span class="math inline">\(\tilde{y_i}\)</span> are themselves unknown and thus we define the <strong>expected</strong> out-of-sample log predictive density,</p>
<p><span class="math display">\[
elpd = expected\ log\ predictive\ density\ for\ a\ new\ \pmb{single}\ data\ \pmb{point}
\]</span></p>
<p><span class="math display">\[
elpd = E_f(log\ p_{post}(\tilde{y_i})) = \int (log\ p_{post}(\tilde{y_i}))\ f(\tilde{y_i})d \tilde{y}\ \ \ \ (1)
\]</span></p>
<p>(<span class="math inline">\(E_f\)</span> denotes the expectation under the true data distribution <span class="math inline">\(f(y)\)</span>).</p>
<p>In the machine learning literature this is often called the mean log predictive density. In any application, we would have some <span class="math inline">\(p_{post}\)</span> but we do not in general know the data distribution <span class="math inline">\(f\)</span>. A natural way to estimate the expected out-of-sample log predictive density would be to plung in an estimate for <span class="math inline">\(f\)</span>, but this will tend to imply too good a fit, as we discuss in Section 3. For now we consider the estimation of predictive accuracy in a Bayesian context.</p>
<p>To keep comparability with the given dataset <!--# EXPLAINED IN THE PARAGRAPH BELOW -->, one can define a measure of predictive accuracy for the <span class="math inline">\(n\)</span> data points taken one at a time:</p>
<p><span class="math display">\[
elppd = expected\ log\ \pmb{pointwise}\ predictive\ density\ for\ a\ new\ data\pmb{set}
\]</span></p>
<p><span class="math display">\[
elppd = \sum_{i = 1}^{n}E_f(log\ p_{post}(\tilde{y_i}))\ \ \ \   (2)
\]</span></p>
<p><strong>which must be defined based on some agreed-upon division of the data</strong> <span class="math inline">\(y\)</span> <strong>into individual data points</strong> <span class="math inline">\(y_i\)</span> <!--# THIS SEEMS TO BE SUPER IMPORTANT FOR HIERARCHICAL MODELS -->. The advantage of using a pointwise measure, rather than working with the joint posterior predictive distribution, <span class="math inline">\(p_{post}(\tilde{y})\)</span> is in the connection of the pointwise calculation to cross-validation, which allows some fairly general approaches to approximation of out-of-sample fit using available data.</p>
<p>2.4 Evaluating predictive accuracy for a fitted model</p>
<p>In practice the parameter <span class="math inline">\(\theta\)</span> is not known, so we cannot know the log predictive density <span class="math inline">\(log\ p(y|\theta)\)</span> <!--# i guess this is the probability density function of y given an exact value of theta, that is, how likely is each possible value of y if theta takes a certain form but there's no uncertainty on the form or value theta takes -->. For the reasons discussed above we would like to work with the posterior distribution, <span class="math inline">\(p_{post}(\theta) = p(\theta | y)\)</span>, and summarize the predictive accuracy of the fitted model <strong>to data</strong> <!--# THIS IS OF OBSERVED DATA, NOT NEW DATA --> by,</p>
<p><span class="math display">\[
lppd = log\ pointwise\ predictive\ density
\]</span></p>
<p><span class="math display">\[
lppd = log \prod_{i=1}^{n} p_{post}(y_i) = \sum_{i = 1}^{n}log \int p(y_i|\theta)\ p_{post}(\theta)d \theta \ \ \ \ (4)
\]</span></p>
<p>(remember that logs turn products into sums)</p>
<p>To compute this predictive density in practice <!--# this seems to be what ben lambert explains above about how to compute the predictive density. Do a comparation between both of them later -->, we can evaluate the expectation using draws from <span class="math inline">\(p_{post}(\theta)\)</span>, the usual posterior simulations, which we label <span class="math inline">\(\theta^s\)</span>, <span class="math inline">\(s = 1,\ …,\ S\)</span>:</p>
<p><span class="math display">\[
\pmb{computed}\ lppd = \pmb{computed}\ log\ pointwise\ predictive\ density
\]</span></p>
<p><span class="math display">\[
= \sum_{i=1}^{n} log\ (\frac{1}{S} \sum_{s= 1}^{S} p(y_i | \theta^s)\ ) \ \ \ (5)
\]</span></p>
<p>(when we compute the lppd, we “replace” the <span class="math inline">\(\int p_{post}(\theta)d\theta\)</span> with <span class="math inline">\(\frac{1}{S}\sum_{s=1}^{S}\)</span>, and we do it <em>pointwise</em> with the <span class="math inline">\(\sum_{i=1}^n\)</span> part)</p>
<p>(remember that this is estimating out of sample predictive accuracy using in sample data and thus is an overestimation of <span class="math inline">\((2)\)</span>. That is, lppd is a biased estimate of elppd)</p>
<p>We typically assume that the number of simulation draws <span class="math inline">\(S\)</span> is large enough to fully capture the posterior distribution; thus we shall refer to the theoretical value <span class="math inline">\((4)\)</span> and the computation <span class="math inline">\((5)\)</span> interchangeably as the “log pointwise predictive density” or lppd of the data.</p>
<p>As we shall discuss in Section 3, the lppd of <strong>observed</strong> data <span class="math inline">\(y\)</span> is an overestimate of the elppd for future data <span class="math inline">\((2)\)</span> <!--# SUPPA IMPORTANT -->. Hence the plan is like to start with <span class="math inline">\((5)\)</span> and then apply some sort of bias correction to get a reasonable estimate of <span class="math inline">\((2)\)</span>. <!--# is this done automatically by the loo package? do we have to do it by hand when computing the exact LFO? see what buerkner does. --></p>
<p>2.5. Choices in defining the likelihood and predictive quantities</p>
<p>As is well known in hierarchical modeling, the line separating prior distribution from likelihood is somewhat arbitrary <!--# WHY --> and is related to the question of what aspects of the data will be changed in hypothetical replications. In a hierarchical model with direct paramenters <span class="math inline">\(\alpha_1,\ ...,\ \alpha_J\)</span> and hyperparameters <span class="math inline">\(\phi\)</span>, factored as <span class="math inline">\(p(\alpha,\phi|y)\ \propto\  p(\phi) \prod_{j = 1}^{J} p(\alpha_j | \phi)\ p(y_j | \alpha_j)\)</span> <!--# p(\alpha_j | \phi)p(\phi) seems to be the prior, whereas p(y_j | \alpha_j) seems to be the likelihood --> we can imagine replicating new data in <strong>existing</strong> groups (with the ‘likelihood’ being proportional to <span class="math inline">\(p(y |\alpha_j)\)</span>) or new data in <strong>new</strong> groups (a new <span class="math inline">\(\alpha_{J+1}\)</span> is drawn, and the ‘likelihood’ is proportional to <span class="math inline">\(p(y | \phi) = \int p(y|\alpha_{J+1})\ p(\alpha_{J+1} | \phi) d\alpha_{J+1}\)</span> <!--# i wrote the equation the other way around because I prefer it like that -->. In either case, we can easily compute the posterior predictive density of the <strong>observed</strong> data <span class="math inline">\(y\)</span>:</p>
<ul>
<li><p>When prediction <span class="math inline">\(\tilde{y} |\alpha_j\)</span> (that is, new data from <strong>existing</strong> groups), we can compute <span class="math inline">\(p(y|\alpha_{j}^s)\)</span> for each posterior simulation <span class="math inline">\(\alpha_{j}^s\)</span> and then take the average, as in <span class="math inline">\((5)\)</span>.</p></li>
<li><p>When predicting <span class="math inline">\(\tilde{y} |\alpha_{J+1}^s\)</span> (that is, new data from a new group), we sample <span class="math inline">\(\alpha_{J+1}^s\)</span> from <span class="math inline">\(p(\alpha_{J+1} |\phi^s)\)</span> to compute <span class="math inline">\(p(y|\alpha_{J+1}^s)\)</span>.</p></li>
</ul>
<p>We are not bothered by the nonuniqueness of the predictive distribution. Just as with posterior predictive checks, different distributions correspond to different potential uses of a posterior inference. <!--# SUPPA IMPORTANT FOR OUR CUSTOM LFO, WHICH I BELIEVE WE WOULD LIKE TO KNOW HOW WELL WE WOULD PREDICT ON A NEW LOCATION GIVEN THE DATA THAT WE CURRENTLY HAVE. MAYBE WE SHOULD HAVE THAT CORRECTIVE TERM GIVEN THE FEW ACTUAL DATA POINTS THAT WE HAVE (DOES A HIERARCHICAL MODEL REDUCE THE "EFFECTIVE" NUMBER OF DATAPOINTS?  -->. Given some particular data, a model might predict new data accurately in some scenarios but not in others <!--# WE HAVE TO DEFINE OUR SCENARIO, do we have one scenario or two? we clearly have the scenario of predicting data from a new location, so thats one. But for now, the scenario of predicting data from inside a location doesnt seem to matter that much. --> .</p>
<p>Vehtari and Ojanen (2012) discuss different prediction scenarios where the future explanatory variable <span class="math inline">\(\tilde{x}\)</span> is assumed to be random, unknown , fixed, shifted, deterministic, or constrained in some way. Here we consider only scenarios with no <span class="math inline">\(x\)</span>, <span class="math inline">\(p(\tilde{x})\)</span> is equal to <span class="math inline">\(p(x)\)</span>, or <span class="math inline">\(\tilde{x}\)</span> is equal to <span class="math inline">\(x\)</span>…</p>
<p>3. Information criteria and effective number of parameters</p>
<!--# problems with IC -->
<p>out-of-sample predictions will typically be less accurate than implied by the within-sample predictive accuracy. To put it another way, the accuracy of a fitted model’s predictions of future data will generally be lower, in expectation, than the accuracy of the same model’s predictions for observed data…</p>
<p>We are interested in prediction accuracy for two reasons: first, to measure the performance of a model that we are using; second, to compare models. Our goal in model comparison is not necessarily to pick the model with lowest estimated prediction error or even to average over condidate models - … - but at least to put different models on a common scale. <strong>Even models with completely different parameterizations can be used to predict the same measurements</strong>.</p>
<p><strong>When different models have the same number of parameters estimated in the same way, one might simply compare their best-fit log predictive densities directly, but when comparing models of differing size or differing effective size</strong> (for example, comparing logistic regressions fit using uniform, spline, or Gaussian process priors), <strong>it is important to make some adjustment for the natural ability of a larger model to fit data better, even if only by chance</strong> <!--# important for comparing our different models -->.</p>
<p>3.1. Estimating out-of-sample predictive accuracy using available data</p>
<p><strong>Several methods are available to estimate the expected predictive accuracy without waiting for out-of-sample data. We cannot compute formulas suchs as</strong> <span class="math inline">\((1)\)</span> <strong>directly because we do not know the true distribution,</strong> <span class="math inline">\(f\)</span><strong>. Instead we can consider various approximations. We know of no approximation that works in general, but predictive accuracy is important enough that it is still worth trying… Each of these methods has flaws, which tells us that any predictive accuracy measure that we compute will be only approximate.</strong></p>
<ul>
<li><p><em>Within-sample predictive accuracy</em>. A naive estimate of the expecte log predictive density for <em>new</em> data is the log predictive density for <em>existing</em> data. As discussed above, we would like to work with the bayesian pointwise formula, that is, lppd as computed using the simulation <span class="math inline">\((5)\)</span>. This summary is quick and easy to understand but is in general an overestimate of <span class="math inline">\((2)\)</span> because it is evaluated on the data from which the model was fit.</p></li>
<li><p><em>Adjusted within-sample predictive accuracy</em>. Given that lppd is a biased estimate of elppd, the nest logical step is to correct that bias. Formulas such as AIC, DIC, and WAIC give approximately unbiased estimates of elppd by starting with something like lppd and then subtracting a correction for the number of parameters, or the effective number of parameters, being fit. These adjustments can give reasonable answers in many cases but have the general problem of being correct at best only in expectation, not necessarily in any given case.</p></li>
<li><p><em>Cross-validation.</em> One can attempt to capture out-of-sample prediction error by fitting the model to training data and then evaluating this predictive accuracy on a holdout set. <strong>Cross-validation avoids the problem of overfitting but remains tied to the data at hand and thus can be correct at best only in expectation</strong>. In addition, cross-validation can be computationally expensive: to get a stable estimate typically requires many data partitions and fits. At the extreme, leave-one-out cross-validation (LOO-CV) requires <span class="math inline">\(n\)</span> fits except when some computation shortcut can be used to approximate the computations.</p></li>
</ul>
<p>3.8. Leave-one-out cross-validation</p>
<p>In Bayesian cross-validation, the data are repeatedly partitioned into a training set <span class="math inline">\(y_{train}\)</span> and a holdout set <span class="math inline">\(y_{holdout}\)</span>, and then the model is fit to <span class="math inline">\(y_{train}\)</span> (thus yielding a posterior distribution <span class="math inline">\(p_{train}(\theta) = p(\theta | y_{train})\)</span>), with this fit evaluated using an estimate of the log predictive density of the holdout data, <span class="math inline">\(log\ p_{train}(y_{holdout}) = log \int p_{pred}(y_{holdout}|\theta)p_{train}(\theta)d\theta\)</span>. Assuming the posterior distribution <span class="math inline">\(p(\theta|y_{train})\)</span> is summarized by <span class="math inline">\(S\)</span> simulation draws <span class="math inline">\(\theta^s\)</span>, we calculate the log predictive density as <span class="math inline">\(log\ (\ \frac{1}{S} \sum_{s=1}^S p(y_{holdout} | \theta^s)\ )\)</span>.</p>
<p>((i guess that <span class="math inline">\(p_{train}(y_{holdout})\)</span> is the predictive density for <span class="math inline">\(y_{holdout}\)</span> induced by the posterior distribution given by the data used to train, and thus the <span class="math inline">\(p_{train}()\)</span> part; this is a “parallel” that is made from the following thing written above “<span class="math inline">\(p_{post}(\tilde{y_i})\)</span> is the predictive density for <span class="math inline">\(\tilde{y_i}\)</span> induced by the posterior distribution <span class="math inline">\(p_{post}(\theta)\)</span>.”))</p>
<p>For simplicity, we will restrict our attention here to leave-one-out cross-validation (LOO-CV), the special case with <span class="math inline">\(n\)</span> partitions in which each holdout set represents a single data point. Performing the analysis for each of the <span class="math inline">\(n\)</span> data points (or perhaps a random subset for efficient computation if <span class="math inline">\(n\)</span> is large) yields <span class="math inline">\(n\)</span> different inferences <span class="math inline">\(p_{post(-1)}\)</span>, each summarized by <span class="math inline">\(S\)</span> posterior simulations, <span class="math inline">\(\theta^{is}\)</span>.</p>
<p>The Bayesian LOO-CV estimate of out-of-sample predictive fit is</p>
<p><span class="math display">\[
lppd_{loo-cv} = \sum_{i = 1}^{n} log\ p_{post(i-1)}(y_i),\ calculated\ as\ \sum_{i=1}^{n}log\ (\frac{1}{S} \sum_{s=1}^{S} p(y_i|\theta^{is})\ )\ \ \ \ (14) 
\]</span></p>
<p>Each prediction is conditiones on <span class="math inline">\(n-1\)</span> data points, which causes underestimation of the predictive fit. For large <span class="math inline">\(n\)</span> the difference is negligible, but for small <span class="math inline">\(n\)</span> (or when using k-fold cross-validation) we can use a first order bias correction <span class="math inline">\(b\)</span> by estimating how much better predictions would be obtained if conditioning on <span class="math inline">\(n\)</span> data points:</p>
<p><span class="math display">\[
b = lppd - \overline{lppd}_{-i}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\overline{lppd}_{-1} = \frac{1}{n} \sum_{i=1}^{n}\sum_{j=1}^{n} log\ p_{post(-i)}(y_j),\ calculated\ as\ \frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{n} log\ (\ \frac{1}{S} \sum_{s=1}^{S} p(y_j | \theta^{is})\ )
\]</span></p>
<p>(don’t really know what the <span class="math inline">\(j\)</span> subindex stands for here)</p>
<p>The bias-corrected Bayesian LOO-CV is then</p>
<p><span class="math display">\[
lppd_{cloo-cv} = lppd_{loo-cv} + b
\]</span></p>
<p>The bias correction <span class="math inline">\(b\)</span> is rarely used as it is usually small, but we include it for completeness.</p>
<p>To make comparisons to other methods, we compute an estimate of the effective number of parameters as</p>
<p><span class="math display">\[
p_{loo-cv} = lppd - lppd_{loo-cv}\ \ \ (15)
\]</span></p>
<p>or, using bias-corrected LOO-CV,</p>
<p><span class="math display">\[
p_{cloo-cv} = lppd - lppd_{cloo}
\]</span></p>
<p><span class="math display">\[ 
= \overline{lppd}_{-i} - lppd_{loo}
\]</span></p>
<p>Cross-validation is like WAIC in that it requires data to be divided into disjoint, ideally conditionally independent, pieces. This represents a limitation of the approach when applied to structured models. In adittion, cross-validation can be computationally expensive except in settings where shortcuts are available to approximate the distributions <span class="math inline">\(p_{post(-i)}\)</span> without having to re-fit the model each time.</p>
<p>Bayesian cross-validation works also with singular models <!--# according to https://arxiv.org/abs/1309.0911 "In this paper, we are concerned with Bayesian information criteria in the context of singular model selection problems, that is, problems that involve models with Fisher-information
matrices that may fail to be invertible".  -->, and Bayesian LOO-CV has been proven to asymptotically equal to WAIC. For finite <span class="math inline">\(n\)</span> there is a difference, as LOO-CV conditions the posterior predictive densities on <span class="math inline">\(n - 1\)</span> data points. These differences can be apparent for small <span class="math inline">\(n\)</span> or in hierarchical models. <!--# i dunno if this is that important because is just about the difference between WAIC and LOO, but we aint doing WAIC, just LFO. --></p>
<p>Other differences arise in regression or hierarchical models. LOO-CV assumes the prediction task <span class="math inline">\(p(\tilde{y_i} | \tilde{x_i}, y_{-i}, x_{-i})\)</span> while WAIC estimates … <!--# included this part only for the equation, the rest is a comparison with WAIC that doesn't seem relevant. --></p>
<p>The cross-validation estimates are similar to the jackknife (Efron and Tibshirani, 1993). Even though we are working with the posterior distribution, our goal is to estimate an expectation averaging over <span class="math inline">\(y^{rep}\)</span> <!--# not explained what yrep is -->in its true, unknown distribution, <span class="math inline">\(f\)</span>; thus, we are studying the frequency properties of a Bayesian procedure.</p>
<p>…</p>
<p>In expectation. As can be seen above, AIC, DIC, WAIC, and <span class="math inline">\(lppd_{loo-cv}\)</span> all are random variables, in that their values depend on the data <span class="math inline">\(y\)</span>, even if the model is known… <!--# maybe should re-check the example. --></p>
<p>…</p>
<!--# few equations evaluating the example -->
<p>…</p>
<p>Despite what the notation might seem to imply, elppd is <em>not</em> the same as E(lppd); the former is the expected log pointwise predictive density for future data <span class="math inline">\(\tilde{y}\)</span>, while the latter is this density evaluated at the observed data <span class="math inline">\(y\)</span> <!--# need to check -->.</p>
<p>The correct ‘effective number of parameters’ (or bias correction) is the difference between <span class="math inline">\(E(lppd)\)</span> and <span class="math inline">\(elppd\)</span>. <!--# more equations below. Might have to check. --></p>
<hr />
<p><u><strong>FROM STONES BOOK</strong></u></p>
<p>2.3 Shannons Desiderata.</p>
<p>One of the properties of Shannons mathematical definition of information is that it is Additive: “The information associated with a set of outcomes is obtained by adding the information of individual outcomes.”</p>
<p>2.4 Information, surprise and Entropy.</p>
<p>The shannon information of an outcome is defined in terms of how “surprising” an outcome is.<!--# see how to relate this to uncertainty. Makes sense to talk about uncertainty of a distribution (does it?) but how do we talk about how uncertain we are about an outcome happening? --> . Since the probability of an outcome is inversely related to how surprised we are when it happens (something very likely to happen is not very surprising, whereas something very unlikely is), we could express the amount of surprise of an outcome value x to be <span class="math inline">\(1/p(x)\)</span>, so that the amount of surprise associated with the outcome value x increases as the probability of x decreases. However, in order to satisfy the additivity condition above, shannon showed that it is better to define surprise as the <span class="math inline">\(log_2 (1/p(x))\)</span> (this is known as the “Shannon information of x”)</p>
<p><strong>Entropy is average shannon information</strong></p>
<p>In practice, we are not usually interested in the surprise of a particular value of a random variable, but we would like to know how much surprise, on avergae, is associated with the entire set of possible values. That is, we would like to know the average surprise defined by the probability distribution of a random variable. The average surprise of a variable <span class="math inline">\(X\)</span> which has a distribution <span class="math inline">\(p(X)\)</span> is called the entropy of <span class="math inline">\(p(X)\)</span>, and is represented as <span class="math inline">\(H(X)\)</span> . Thus, the entropy is just the average shannon information. E.g., if we have a sequence of outcomes <span class="math inline">\((x_1,\ ..., \ x_n)\)</span>. then the entropy or shannon information of that sequence is:</p>
<p><span class="math display">\[
H(X) \approx \frac{1}{n} \sum_{i = 1}^{n} log \frac{1}{p(x_i)}
\]</span></p>
<p>TO BE CONTINUED…. (page 34)</p>
<p>p79. The uncertainty we have about the value of Y is initially summarised by its entropy H(Y)</p>
<p>p84. Just as the entropy of a single variable (with finite bounds) can be considered to be a measure of its uniformity <!--# dont really get what he means with uniformity, because "uniformity" seems more like something related to variance rather than something related to an average -->, so the entropy of a joint distribution is also a measure of its uniformity (provided X and Y lie within a fixed range).</p>
<hr />
<p>wiki KL divergence <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" class="uri">https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence</a></p>
<p>In <a href="https://en.wikipedia.org/wiki/Mathematical_statistics" title="Mathematical statistics">mathematical statistics</a>, the <strong>Kullback–Leibler divergence,</strong> <span class="math inline">\(D_{KL}\)</span> (also called <strong>relative entropy</strong>), is a measure of how one <a href="https://en.wikipedia.org/wiki/Probability_distribution" title="Probability distribution">probability distribution</a> is different from a second, reference probability distribution… In contrast to <a href="https://en.wikipedia.org/wiki/Variation_of_information" title="Variation of information">variation of information</a>, it is a distribution-wise <em>asymmetric</em> measure and thus does not qualify as a statistical <em>metric</em> of spread – it also does not satisfy the <a href="https://en.wikipedia.org/wiki/Triangle_inequality" title="Triangle inequality">triangle inequality</a>. In the simple case, a relative entropy of 0 indicates that the two distributions in question have identical quantities of information. In simplified terms, it is a measure of surprise…</p>
<p>Consider two probability distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>. Usually, <span class="math inline">\(P\)</span> represents the data, the observations, or a measured probability distribution. Distribution <span class="math inline">\(Q\)</span> represents instead a theory, a model, a description or an approximation of <span class="math inline">\(P\)</span>. The KL divergence is then interpreted as the average difference of the number of bits required for enconding samples of <span class="math inline">\(P\)</span> using a code optimized for <span class="math inline">\(Q\)</span> rather than one optimized for <span class="math inline">\(P\)</span>.</p>
<p>Various conventions exist for referring to <span class="math inline">\(D_{KL}(P || Q)\)</span> in words. Often it is referred to as the divergence between <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>, but this fails to convey the fundamental asymmetry in the relation. Sometimes, it may be described as the divergence of <span class="math inline">\(P\)</span> from <span class="math inline">\(Q\)</span> or as the divergence from <span class="math inline">\(Q\)</span> to <span class="math inline">\(P\)</span>. This reflects the asymmetry in Bayesian inference, which starts from a prior <span class="math inline">\(Q\)</span> and updates to the posterior <span class="math inline">\(P\)</span>. Another common way to refer to <span class="math inline">\(D_{KL}(P || Q)\)</span> is as the relative entropy of <span class="math inline">\(P\)</span> with respect to <span class="math inline">\(Q\)</span>.</p>
<p>The relative entropy from <span class="math inline">\(Q\)</span> to <span class="math inline">\(P\)</span> is often denoted <span class="math inline">\(D_{KL}(P || Q)\)</span>.</p>
<p>In the context of machine learning, <span class="math inline">\(D_{KL}(P || Q)\)</span> is often called the infromation gain achieved if <span class="math inline">\(P\)</span> would be used instead of <span class="math inline">\(Q\)</span> which is currently used. By analogy with information theory, it is called the relative entropy of <span class="math inline">\(P\)</span> with respect to <span class="math inline">\(Q\)</span>.</p>
<p>Expressed in the language of Bayesian inference, <span class="math inline">\(D_{KL}(P || Q)\)</span> is a measure of the information gained by revising one’s beliefs from the prior probability distribution <span class="math inline">\(Q\)</span> to the posterior probability distribution <span class="math inline">\(P\)</span>. In other words, it is the amount of information lost when <span class="math inline">\(Q\)</span> is used to approximate <span class="math inline">\(P\)</span>. In applications. <span class="math inline">\(P\)</span> typically represents the “true” distribution of data, observations, or a precisely calculated theoretical distribution, while <span class="math inline">\(Q\)</span> typically represents a theory, model, description, or approximation of <span class="math inline">\(P\)</span>. In order to find a distribution <span class="math inline">\(Q\)</span> that is closest to <span class="math inline">\(P\)</span>, we can minimize the KL divergence and compute an information projection.</p>
<p>Although it is often intuited as a way of measuring the distance between probability distributions, the KL divergence is not a true metric. It does not obey the Triangle Inequality, and in general <span class="math inline">\(D_{KL}(P || Q)\)</span> does not equal <span class="math inline">\(D_{KL}(Q || P)\)</span>.</p>
<p>Arthur Hobson proved that relative entropy is the only measure of difference between probability distributions that satisfies some desired properties, which are the canonical extension to those appearing in a commonly used characterization of entropy. Consequently, mutual information is the only measure of mutual dependence that obeys certain related conditions, since it can be defines in terms of the KL divergence.</p>
<p>Properties of the KL divergence:</p>
<ul>
<li><p>It is always non-negative <span class="math inline">\(D_{KL}(P || Q) \geq 0\)</span>, a result known as “Gibbs inequality”, with <span class="math inline">\(D_{KL}(P || Q)\)</span> equal to zero iff <span class="math inline">\(P = Q\)</span> almost everywhere.</p></li>
<li><p>No upper-bound exists for the general case. However, it is shown that if <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> are two discrete probability distributions built by distributing the same discrete quantity, then the maximum value of <span class="math inline">\(D_{KL}(P || Q)\)</span> can be calculated.</p></li>
<li><p>KL divergence remains well-defined for continuous distributions, and furthermore is invariant under parameter transformations.</p></li>
<li><p>KL divergence is additive for independent distributions, in much the same way as Shannon entropy.</p></li>
<li><p>There are 2 more but were skipped because of lack of minimal understanding.</p></li>
</ul>
<p>One might be tempted to call relative entropy a “distance metric” on the space of probability distributions, but this would not be correct as it is not symmetric (i.e., <span class="math inline">\(D_{KL} (P || Q) \neq D_{KL}(Q||P)\)</span> ) nor does it satisfy the triangle inequality.</p>
<hr />
<p>Straight from JAGS user manual 2017 (<a href="https://sourceforge.net/projects/mcmc-jags/files/Manuals/4.x/" class="uri">https://sourceforge.net/projects/mcmc-jags/files/Manuals/4.x/</a>).</p>
<p>START OF EXCERPT</p>
<p>6.3. Functions associated with distribution</p>
<p>All distribution sin JAGS have a log density function associated withi them. For example, if variable <span class="math inline">\(x\)</span> has a normal distribution with mean <span class="math inline">\(mu\)</span> and precision <span class="math inline">\(tau\)</span>:</p>
<p><span class="math display">\[
x ~ \sim dnorm(mu, tau)
\]</span></p>
<p>then the log density of <span class="math inline">\(x\)</span> is given by</p>
<p><span class="math display">\[
ldx \leftarrow logdensity.norm(x, mu, tau)
\]</span></p>
<p>In general, if “dfoo” is the name of a distribution then the associated log density function is “logdensity.foo”. The first argument of the log-density function is the sample value at which the log-density is evaluated and the remaining arguments are the parameters of the distribution <!--# posterior parameters i'd like to think -->.</p>
<p>FINISH OF EXCERPT</p>
<p>Maybe say that u r not entirely sure if this is giving u what u need (the “posterior” log likelihood or posterior predictive density), and that u still need to check JAGS codebase to make sure because it’s not clearly explained in the user manual.</p>
<p>search for the first time “density” comes up in the manual</p>
<hr />
<p>THINGS TO SAY AT SOME TIME.</p>
<ul>
<li><p>for a sneak peak into information theory, check colah’s introduction.</p></li>
<li><p>for a deeper dive, check “Information Theory: A tutorial introduction” by James V. Stone.</p></li>
<li><p>check “students guide to bayesian statistics” by Ben Lambert for more info on ic, cv, and predictive accuracy in general</p></li>
<li><p>why is the loglikelihood the measure of a model goodness of fit? 1. to be explained when deeply understood… (may be deeply understood after reading</p></li>
<li><p>say at the end of a note or explanation when u believe the explanation is not yet fully satisfying.</p></li>
<li><p>Why not use IC such as AIC, DIC or WAIC instead? NOTE explaining. Maybe do something simple. Remember to include that AIC, DIC and WAIC all have the same “““pattern/structure”““” of being the modelFit - penalty (check on bens vid if this is correct)</p></li>
<li><p>is there a difference when using complete, partial or non pooled data? note to explain and cite.</p></li>
<li><p>DISCLAIMER: since these are only my personal notes posted online, some things are written by me, whereas others are just excerpts from the bibliography I cite. For simplicity, I avoid being too formal with the citations, but remember that some paragraphs may be a frankenstein’s monster type mixture of other sources. Maybe say that if you find a sentence or paragraph that you feel that clearly explains something, you just leave it as it is (or clone it). But many times that’s not how I feel about an explanation, so I change something previously written by someone else or I write something entirely new.</p></li>
<li><p>Say somewhere that corrections or other type of constructive criticism is welcome (maybe check how colah says this), and maybe dont say it until you post it online.</p></li>
<li><p>Ben Lambert explains X in a friendly way, but for a more rigorous explanation check X paper (for example, in the case of measures of predictive accuracy and IC-CV)</p></li>
<li><p>Explain at the top how to “clone” the project (that is, download the entire project) from github and how to install the packages, use renv, docker, etc. Maybe point to the docker and git pages for installation guidelines. Explain that the project will be downloaded in whatever folder you are in when using the terminal. Also can point to the github website where it explains how to clone a project without the terminal.</p></li>
<li><p>Say that if opening with the latest Rstudio version, remember to switch to the visual editor.</p></li>
<li><p>Make some sort of introduction (maybe with an index) saying what the document is going to talk about and why (with “why” i mean something like “we are going to learn X theory in order to be able to understand Y, and then use that to define the steps to do the LFO.”</p></li>
<li><p>Maybe do boxes in between the paragraphs instead of notes at the end, and explain at the beginning that there will be boxes diving deeper on some of the terms or concepts but are not needed to follow how to do the exact LFO, so feel free to skip them.</p></li>
<li><p>Maybe say something like “if you are not comfortable with expectations, I’d suggest you read appendix E from stone’s introduction to information theory.”</p></li>
<li><p>Should I send this to distill or to the stan forum?</p></li>
<li><p>Maybe publish it on medium, with a name something like “model comparison: a bayesian and information theoretic perspective”.</p></li>
<li><p>Maybe the order of the post should be: state the problem (estimate out of sample predictive accuracy), introduce the idea of f(xnew) vs p(xnew|x), do an information theory introduction, show the KL divergence, the ELPD and the EELPD, explain how cross validation comes natural after this ELPD (maybe explain how IC were attempts estimate what now CV does because of the small amount of data and computational power that used to be available), then explain how we would do the basic LOO-CV, then state the problem of using LOO-CV with time series, and then state the solution of LFO-CV.</p></li>
<li><p>note or box: Where does the KL divergence comes from? and in a line below: to be explained when deeply understood… or: More on this when deeply understood…</p></li>
<li><p>say that the document is yet not polished so there may be “gramatical?” mistakes.</p></li>
<li><p>explain averages from distributions in a box, probably at the beggining, or just say that X appendix on stones book does a great explanation. Maybe check the appendix on probability rules and if it’s good then say the same thing.</p></li>
<li><p>Ben lambert does a friendly introduction to the topic of evaluating the predictive performance of a model (chapter 10), but gelman <span class="math display">\[14\]</span> does a thorough explanation of the topic.</p></li>
<li><p>maybe explain the KL thing and when you say that the second part is the elpd, make a new section explaining where does the elpd (or elppd, cant remember) comes from.</p></li>
<li><p>make a section explaining the difference when the data is complete, no or partially pooled.</p></li>
<li><p>First explain LOO-CV and then go on to explaining LFO-CV</p></li>
<li><p>con criterio y no en una principled way.</p></li>
<li><p>Say that you tested 3 different models, each one with an increasing degree of complexity, and the middle model was the one with the highest predictive accuracy.</p></li>
<li><p>maybe recommend michael betancourt writings for learning probability instead of stones appendix (which may be good but it’s surely very short).</p></li>
<li><p>when talking about KL divergence, maybe it’s better to say: not yet fully understood, more on this later.</p></li>
<li><p>Maybe insert the word “GAP” where you know there’s a gap in your explanation, and explain at some point that you will fill them up as you understand it more.</p></li>
<li><p>say: for a deeper dive YOU CAN CHECK stones book (not, you should check or I recommend, cuz I dont really have the knowledge to make such a statement).</p></li>
</ul>
<p>DUDAS:</p>
<ul>
<li>what is jags doing with that loglikelihood function? does it give u a different thing when using a hierarchical vs a non-hierarchical model? maybe check the forums or directly read the source code (but since it’s probably in c++ and java, its going to take a while to figure it out and im not really willing to learn c++ given that julia is already here). Also search for “JAGS cross validation” for more general info. If you dont find good info about what the heck is that loglikelihood doing (is it the “posterior” log likelihood? or the “prior” loglikelihood?, that is, is it giving me the log posterior predictive density (also called “log likelihood”) or not?). Maybe it’s just simpler to compute the proper posterior predictive density on my own. Or maybe see what that extract_loglikelihood() function from the loo package does.</li>
</ul>


<!-- Adjust MathJax settings so that all math formulae are shown using
TeX fonts only; see
http://docs.mathjax.org/en/latest/configuration.html.  This will make
the presentation more consistent at the cost of the webpage sometimes
taking slightly longer to load. Note that this only works because the
footer is added to webpages before the MathJax javascript. -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>




</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
